
    This script can be used to query the StackExchange data downloaded from http://nlp.cis.unimelb.edu.au/resources/cqadupstack/.

    The script contains a main function called load_subforum(). It has one argument: a StackExchange subforum.zip file.
    load_subforum() uses this file to create a 'Subforum' object and returns this.
    Alternatively, you can make a subforum object directly by calling the class 'Subforum' yourself. Just like load_subforum() it needs a zipped subforum file as its only argument.  

    Subforum objects can be queried using the following methods:

  
 |  average_ndcg_at(self, scorefile, cutoff=None, include_related_posts=False)
 |      Takes a file with scores and a cutoff point as input and returns the Normalised Discounted Cumulative Gain at the cutoff point. The default is 10.
 |      If the optional argument 'include_related_posts' is set to True, then related posts are treated as half relevant. 
 |      See "Cumulated Gain-based Evaluation of IR Techniques" by Jarvelin and Kekalainen 2002 for more information on this metric.
 |      Only queries with relevant posts are taken into account. Queries with ONLY related posts are ignored, even with include_related_posts=True, to make the scores better comparable. 
 |      Just for extra information: Taking related posts into account can lower the score if these documents are not returned, because they appear in the ideal ranking that is used in the calculation of the metric. This is not the case for the other retrieval evaluation metrics.
 |  
 |  aerge_precision_at(self, scorefile, cutoff=None, include_related_posts=False)
 |      Takes a file with scores and optionally a cutoff point as input and returns the Average Precision (at this cutoff, if specified). 
 |      If the optional argument 'include_related_posts' is set to True, then related posts are treated as half relevant.
 |      Only queries with relevant posts are taken into account. Queries with ONLY related posts are ignored, even with include_related_posts=True, to make the scores better comparable.
 |
 |  average_recllat(self, scorefile, cutoff=None, include_related_posts=False)
 |      Takes a file with scores and optionally a cutoff point as input and returns the Recall (at this cutoff, if specified).
 |      If the optional argument 'include_related_posts' is set to True, then related posts are treated as half relevant.
 |      Only queries with relevant posts are taken into account. Queries with ONLY related posts are ignored, even with include_related_posts=True, to make the scores better comparable.
 |
 |  change_to_default_sopords(self, stopwordset='middle')
 |      Changes the stopword list to one of the supplied ones: 'nltk', 'indri', 'short' or 'middle'. 'Middle' is the default.
 |      The NLTK stopword list contains 127 stopwords. (http://www.nltk.org/book/ch02.html#code-unusual)
 |      The Indri stopword list contains 418 stopwords. (http://www.lemurproject.org/stopwords/stoplist.dft)
 |      Short = ["a", "an", "the", "yes", "no", "thanks"]
 |      Middle = ["in", "on", "at", "a", "an", "is", "be", "was", "I", "you", "the", "do", "did", "of", "so", "for", "with", "yes", "thanks"]
 |      To be able to use the NLTK stopwords, they need to be downloaded first. See: http://www.nltk.org/data.html for more info.
 |      If the data is not downloaded first, the script will default to the NLTK stopword list of November 2015.
 |
 |  evaluate_classification(self, scorefile)
 |      Takes a file with scores as input and returns a dictionary with the Precision, Recall, F1-score, Accuracy and precision and Recall per class.
 |      The file with scores should have the same format as the classification training and test sets:
 |      One line per classification with two space separated postids followed by a 1 for duplicates, or 0 for non-duplicates.
 |
 |  get_acceptedanswer(self, postid)
 |      Takes a post id as input and returns the answer id of the accepted answer if it exists, else it returns False.
 |
 |  get_acceptedanswer_date(self, answerid)
 |      Takes an answer id as input and returns the date at which it was selected as the best answer in YYYY-MM-DD format, if it exists. Else it returns 0.
 |
 |  get_all_duplicate_pairs(self)
 |      Takes no input an returns a list of duplicate question pairs as tuples of ids.
 |
 |  get_all_postids(self)
 |      Takes no input and returns a list of ALL post ids.
 |
 |  get_all_related_pairs(self)
 |      Takes no input an returns a list of related question pairs as tuples of ids.
 |
 |  get_all_uses(self)
 |      Takes no input and returns a list of all users.
 |
 |  gt_answer_commentcount(self, answerid)
 |      Takes an answer id as input and returns and integer representing the number of comments this answer has received.
 |
 |  ge_aswer_comments(self, answerid)
 |      Takes an answer id as in put and returns a list of comment ids.
 |
 |  get_answer_parentid(self, answerid)
 |      Takes an answer id as input and returns its parent id: the id of the post it is an answer of.
 |
 |  gt_answerbody(self, answerid)
 |      Takes an answer id as input and returns the body of the answer. That is the text of the answer.
 |
 | get_answercount(self, postid)
 |      Takes a post id as input and returns an integer representing the number of answers it has received.
 |
 |  getanswerdate(self, answerid)
 |      Takes an answer id as input and returns the date the answer was posted in YYYY-MM-DD format.
 |
 |  get_answers(self, postid)
 |      Takes a post id as input and returns a list of answer ids.
 |
 |  get_answescre(self, answerid)
 |      Takes an answer id as input and returns an integer representing the score of the answer. This is the number of upvotes minus the number of downvotes is has received.
 |
 |  get_answrtime(self, answerid)
 |      Takes an answer id as input and returns the time the answer was posted in HH:MM:SS format.
 |
 |  et_answeruserid(self, answerid)
 |      Takes an answer id as input and returns the userid of the person that posted it. Returns False if the user is not known.
 |
 |  get_comment_parentid(self, commentid)
 |      Takes a comment id as input and returns its parent id: the id of the post or answer it is a comment to.
 |
 |  get_comment_parenttype(self, commentid)
 |      Takes a comment id as input and returns either 'question' or 'answer', depending on the type of its parent id.
 |
 |  get_commentbody(self, commentid)
 |      Takes a comment id as input and returns the body of the comment.
 |
 |  get_commentdate(self, commentid)
 |      Takes a comment id as input and returns the date the comment was posted, in YYYY-MM-DD format.
 |
 |  get_commentscore(self, commentid)
 |      Takes a comment id as input and returns an integer representing the score of the comment. This is the number of upvotes minus the number of downvotes is has received.        
 |
 |  get_commettme(self, commentid)
 |      Takes a comment id as input and returns the time the comment was posted, in HH:MM:SS format.
 |
 |  get_commentuserid(self, commentid)
 |      Takes a comment id as input and returns the id of the user that posted the comment.
 |
 |  get_duplicates(self, postid)
 |      Takes a post id as input and returns a list of ids of posts that have been labeled as a duplicate of it.
 |
 |  get_duptagdates(self, postid1, postid2)
 |      Takes two post ids as input and returns a list of dates on which this pair received a duplicate tag, in %Y-%m-%dT%H:%M:%S.%f format.
 |      Usually the list only contains one date, but sometimes it contains multiple.
 |
 |  get_dupvotrs(self, postid1, postid2)
 |      Takes two post ids as input and returns a list of the users that have voted for these two questions to be duplicates.
 |
 |  get_fistduptagdate(self, postid)
 |      Takes one postid as input and returns the date on which this post was tagged as a duplicate for the first time, in %Y-%m-%dT%H:%M:%S.%f format,
 |      or 0 if the post has not been tagged as a duplicate (yet).
 |
 |  get_older_posts(self, postid)
 |      Takes a post id as input and returns a list of question ids that are older than the input post id.
 |
 |  get_ordered_list_of_posts(self)
 |      Takes no input and returns a list of tuples (postid, datetime object), ordered chronologically from newest to oldest post.
 |
 |  get_post_commentcount(self, postid)
 |      Takes a post id as input and returns and integer representing the number of comments this post has received.
 |
 |  et_post_comments(self, postid)
 |      Takes a post id as input and returns a list of comment ids.
 |
 |  get_post_title_and_body(self, postid)
 |      Takes a post id as input and returns the title and the body of the post together as one string, so in other words, the full initial post.
 |
 |  get_postbody(self, postid)
 |      Takes a post id as input and returns the body of the post.
 |
 |  get_postdae(self, postid)
 |      Takes a post id as input and returns the date the post was posted in YYYY-MM-DD format.
 |
 | get_postfavoritecount(self, postid)
 |      Takes a post id as input and returns an integer representing the nr of times this post has been favoured by a user.
 |      More information on what that means can be found here: http://meta.stackexchange.com/questions/53585/how-do-favorite-questions-work
 |
 |  get_posts_dups_nodups_ad_elated(self)
 |      Takes no input and return three lists: one with all posts that have at least one duplicate, one with all posts that have at least one related question, and one with all posts that don't have any duplicates or related questions. In that order.
 |      Calling this method is quicker than calling get_posts_with_duplicates(), followed by get_posts_with_related(), followed by get_posts_without_duplicates() if you want all three types of questions.
 |      There may be overlap in the list of posts with duplicates and posts with related questions, because posts can have bot duplicates and related questions.
 |
 |  get_posts_with_and_without_duplicates(self)
 |      Takes no input and returns two lists: one with all posts that have at least one duplicate, and one with all posts that don't have any duplicates. In that order.
 |      Calling this method is quicker than calling get_posts_with_duplicates() followed by get_posts_without_duplicates() if you want both dups and non-dups.
 |
 |  get_posts_with_duplicates(self)
 |      Takes no input and returns a list of all posts that have at least one duplicate.
 |
 |  get_posts_with_related(self)
 |      Takes no input and returns a list of all posts that have related questions.
 |
 |  gt_posts_without_duplicates(self)
 |      Takes no input and returns a list of all posts that don't have any duplicates.
 |
 |  get_postscore(self, postid)
 |      Takes a post id as input and returns the score of the post. This is the number of upvotes minus the number of downvotes is has received.
 |
 |  get_posttags(self, postid)
 |      Takes a post id as input and returns a list of tags.
 |
 |  get_posttime(self, postid)
 |      Takes a post id as input and returns the time the post was posted in HH:MM:SS format.
 |
 |  get_posttile(self, postid)
 |      Takes a post id as input and returns the title of the post.
 |
 |  get_postuserid(self, postid)
 |      Takes a post id as input and returns the userid of the person that posted it. Returns False if the user is not known.
 |
 |  get_postviewcount(self, postid)
 |      Takes a post id as input and returns the number of times the post has been looked at by users.
 |
 |  get_random_pair_of_posts(self)
 |      Takes no input and returns a tuple with two random post ids and a duplicate verdict. The second is always lower than the first.
 |      Example: (4865, 553, 'dup')
 |      Other values for the verdict are: 'related' and 'nondup'.
 |
 |  get_rndm_postid(self)
 |      Takes no input and returns a random post id.
 |
 |  get_related(self, postid)
 |      Takes a post id as input and returns a list of ids of posts that have been labeled as related to it.
 |
 |  get_true_label(self, postid1, postid2)
 |      Takes two postids as input and returns the true label, which is one of "dup", "nodup" or "related".
 |
 |  get_user_age(self, userid)
 |      Takes a user id as input and outputs the user's age as an integer, if known. Else it returns 'unknown'.
 |
 |  get_user_answers(self, userid)
 |      Takes a user id as input and returns a list of the answers he/she has written.
 |
 |  get_user_badges(self, userid)
 |      Takes a user id as input and returns a list of the badges this user has earned.
 |      Information on what badges are and which ones can be earned can be found here: http://stackoverflow.com/help/badges
 |
 |  get_user_downvotes(self, userid)
 |      Takes a user id as input and outputs an integer representing how many downvotes on posts or answers this user has received.
 |
 |  get_user_joindate(self, userid)
 |      Takes a user id as input and outputs the date this user joined this subforum, in YYYY-MM-DD format.
 |
 |  get_user_lataccess(self, userid)
 |      Takes a user id as input and outputs the last time this user has logged into this subforum, in YYYY-MM-DD format.
 |
 |  get_user_posts(self, userid)
 |      Takes a user id as input and returns a list of the question posts he/she has made.
 |
 |  ge_uer_reputation(self, userid)
 |      Takes a user id as input and outputs an integer representing the reputation of the user.
 |      Information on what this means and how it is calculated can be found here http://stackoverflow.com/help/whats-reputation
 |
 |  get_user_upvotes(self, userid)
 |      Takes a user id as input and outputs an integer representing how many upvotes on posts or answers this user has received.
 |
 |  et_user_views(self, userid)
 |      Takes a user id as input and outputs an integer representing how often people have viewed a post by this user.
 |
 |  mean_averageprcision(self, scorefile, include_related_posts=False)
 |      Takes a file with scores as input and returns the Mean Average Precision (MAP) score.
 |      If the optional argument 'include_related_posts' is set to True, then related posts are treated as half relevant.
 |      Only queries with relevant posts are taken into account. Queries with ONLY related posts are ignored, even with include_related_posts=True, to make the scores better comparable.
 |
 |  mean_reciprocal_rank(self, scorefile)
 |      Takes a file with scores as input and returns the Mean Reciprocal Rank (MRR) score.
 |
 |  perform_cleaning(self, s, maxcodelength=150, remove_stopwords=False, remove_punct=False, stem=False)
 |      Takes a string as input and returns a cleaned version.
 |      - The string will be lowercased and newlines removed.
 |      - HTML tags will be removed.
 |      - Mentions of possible duplicates will be removed.
 |      - URLs pointing to other StackExchange threads are turned into 'stackexchange-url'.
 |      - Blocks of code will be removed.
 |      - Contracted forms will be expanded. E.g. "didn't" --> "did not".
 |      - '&amp;' will be turned into 'and'.
 |      - Other HTML entities will be removed, and string matching the following pattern too: '&#?[a-z]+;'.
 |      - Whitespace is added around punctuation
 |      OPTIONAL ARGUMENTS:
 |      maxcodelength: the maximum length of code blocks that will not be removed. Default: 150.
 |      remove_stopwords: removed stop words. (Values: True or False)
 |      remove_punct: punctuation is removed, except for punctuation in URLs and numbers. (Values: True or False)
 |      stem: stemming is performed via the Porter stemmer as implemented in the NLTK (http://www.nltk.org/). (Values: True or False)
 |
 |  plot_roc(self, scorefile, plotfilename)
 |      Takes a file with scores and a the name of a plot file (png) as input and returns the false positive rates (list), true positive rates (list), thresholds at which they were computed (list) and the area under the curve (float). The plot will be written to the supplied plot file.
 |      The scores can either be probability estimates of the positive class, confidence values, or binary decisions.
 |      This method requires scikit-learn to be installed: http://scikit-learn.org/stable/install.html
 |      This method only computes the ROC curve for the positive class. See http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html for an example on how to make curves for multiple classes (for instance when you have a third class for the related questions).
 |
 | split_for_classification(self, outputdir='.')
 |      Takes a directory as input and writes twelve plain text files to this directory: a small test set, a large test set, and 10 files for the training set (trainpairs_[01-10].txt, testpairs_small.txt and testpairs_large.txt).
 |      Each line in these sets contains two postids and a label (1 for duplicate, 0 for non-duplicate), separated by a space. Each of these pairs is a training or test instance.    
 |      The training pairs have been divided over ten different files, each with a similar class distribution. These can be used for ten-fold cross-validation.
 |
 |      To make the split all posts are ordered according to date. Next the set is cut into two at a certain date.
 |      This date is chosen such that the test set will ideally contain at least 200 duplicate pairs, or if that iss not possible, as many as possible, with a minimum of 100, and the train set contains at least four times as many.
 |      The test set contains pairs of posts with a date after the cutoff date. Posts are only combined with older posts, as would be the case in a real world setting. The training set contains pairs of posts with a date before the cutoff date. Again, posts are only combined with older posts.
 |      A consequence of this approach is that we lose a number of duplicate pairs, namely the ones that are posted after the cutoff date, but their duplicate was posted before.     
 |      Both testpairs_large.txt and the trainpairs files will contain millions of pairs.
 |      Testpairs_small.txt contains a subset of testpairs_large.txt. It is a smaller and more balanced set, which contains ten times more non-duplicate pairs than duplicate pairs.  
 |
 |  split_for_retrieval(self)
 |      Takes no input and returns three lists: one with test ids, one with development ids and one with ids to be indexed.
 |      The test and development sets contain the most recent posts in the subforum, such that each contains about 15% of all the posts that have duplicates.
 |      They have been assigned alternately to the test and devel sets, so they are quite similar.
 |      Both test and development sets also contain posts that do not have any duplicates, in the actual proportion of the particular subforum.
 |
 |  strip_tags(self, html)
 |      # Source: http://stackoverflow.com/questions/753052/strip-html-from-strings-in-python
 |
 |  supply_stopwords(self, filename)
 |      Takes as input a plain text file encoded in UTF-8 with one stop word per line and saves these internally in a stop word list.
 |      This list will be used in cleaning if perform_cleaning() is called with remove_stopwords=True.
 |
 |  tokenize(self, s)
 |      Takes a string as input, tokenizes it using NLTK (http://www.nltk.org) and returns a list of the tokens.
 |
 |  truncated_mean_average_precision(self, scorefile, include_related_posts=False)
 |      Takes a file with scores as input and returns the Truncated Mean Average Precision (TMAP) as explained in Liu et al. 2016 (http://people.eng.unimelb.edu.au/tbaldwin/pubs/sigir2016.pdf).
 |      If the optional argument 'include_related_posts' is set to True, then related posts are treated as half relevant. The default value is False.
 |      Only queries with relevant posts are taken into account. Queries with ONLY related posts are ignored, even with include_related_posts=True, to make the scores better comparable.
 |
 |  url_cleaning(self, s)
 |      Takes a string as input and removes references to possible duplicate posts, and other stackexchange urls.
 |
 |  very_basic_cleaning(self, s)
 |
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |
 |  __dict__
 |      dictionary for instance variables (if defined)
 |
 |  __weakref__
 |      list of weak references to the object (if defined)
 |
 |  stopwords
 |      Returns the current list of words that is used as the stop word list. It can be accessed via self.stopwords


    The input to the evaluation metric methods (the scorefile), should be a plain text file with one query per line in the following format:

    queryid1    result1 result2 result3 etc.
    queryid2    result1 result2 result3 etc.

    Both the query id and results should be post ids. They should be separated either by a space or a TAB.

    -----------------------------

    Here are some examples of how to use the script:

    >>> import query_cqadupstack as qcqa
    >>> o = qcqa.load_subforum('/home/hoogeveen/datasets/CQADupStack/webmasters.zip')
    >>> testset, develset, indexset = o.split_for_retrieval()
    >>> len(develset)
    1862
    >>> for i in develset:
    ...     if o.get_duplicates(i) != []:
    ...         print i, o.get_duplicates(i)
    ...
    69050 [u'8710']
    68979 [u'52812']
    68897 [u'6073']
    68856 [u'6073']
    68689 [u'20838']
    etc.
    >>> o.get_posttitle('18957')
    u'What do you consider a "mobile" device?'
    >>> o.get_postbody('18957')
    u'<p>I'm implementing a mobile-friendly version of our corporate web site and will be using <a href="http://wurfl.sourceforge.net/" rel="nofollow" title="WURFL">WURFL</a> to detect mobile browsers and redirect them to our mobile site.  Having recently purchased an Android tablet, I've found that many sites consider it to be a mobile device even though it has a large 10" screen and it's perfectly capable of handling sites designed using standard desktop resolutions.</p>

<p>My plan is to use WURFL, examine the device capabilities and treat anything with a resolution width of less than 700px as a mobile device, but I'd like some input as to that sweet spot for determining mobile vs desktop.</p>
'
    >>> o.perform_cleaning(o.get_postbody('18957'))
    u'i am implementing a mobile-friendly version of our corporate web site and will be using wurfl to detect mobile browsers and redirect them to our mobile site . having recently purchased an android tablet , i have found that many sites consider it to be a mobile device even though it has a large 10" screen and it is perfectly capable of handling sites designed using standard desktop resolutions . my plan is to use wurfl , examine the device capabilities and treat anything with a resolution width of less than 700px as a mobile device , but i would like some input as to that sweet spot for determining mobile vs desktop .'
    >>> for a in o.get_answers('18957'):
    ...     print a, o.get_answerscore(a)
    ...
    18985 1
    18980 0
    >>> o.get_posttags(o.get_random_postid())
    [u'wordpress', u'redirects', u'blog', u'plugin']
    >>> o.get_postuserid('18957')
    u'1907'
    >>> o.get_user_posts('1907')
    [u'18957']
    >>> o.get_user_views('1907')
    6
    >>> o.stopwords
    ['in', 'on', 'at', 'a', 'an', 'is', 'be', 'was', 'I', 'you', 'the', 'do', 'did', 'of', 'so', 'for', 'with']
    >>> o.supply_stopwords('new_stopwords_testfile.txt')
    >>> o.stopwords
    [u'new_stopword1', u'new_stopword2', u'new_stopword3']
    >>> o.change_to_default_stopwords()
    >>> o.stopwords
    ['in', 'on', 'at', 'a', 'an', 'is', 'be', 'was', 'I', 'you', 'the', 'do', 'did', 'of', 'so', 'for', 'with']

    -----------------------------

    Please see the README file that came with this script for more information on the data.
